---
title: ''
---

<div style="margin-bottom:50px;">
</div>

```{r echo=FALSE, out.width='100%', fig.align="center"}
knitr::include_graphics("E:\\Dropbox\\GitHub\\chemstat-r-github.io\\Image\\PNG_FILE_02\\data_wragling.png")
```

<div style="margin-bottom:20px;">
</div>


### **Table of Content**

* [Data Wrangling](#data-wrangling)

* [Important R Packages for Data Wrangling](#important-r-rackages-for-data-wrangling)

* [Data Processing with dplyr and tidyr](#data-processing-with-dplyr-tidyr)

  - [Pipe Operator](#Pipe-Operator) 

  - [Join](#jOIN)

  - [Filter](#filter)

  - [Select](#select)

  - [Rename](#rename)

  - [Arrange](#arrange)

  - [Combined multiple functions with Pipes](#combined-multiple-functions-with-pipes)

  - [Pivoting dataframe](#pivoting-dataframe)

  - [Mutate]((#mutate))

  - [summarize](#summarize)

  - [Group by](#group-by)
  
* [Cleaning a Messy Data](#cleaning-a-messy-data) 

* [Further Reading](#further-reading)

<div style="margin-bottom:60px;">
</div>


##  **Data Wrangling**

**Data wrangling**, also known as **data munging**, is a series of assembling, cleaning, and transforming processes to understand data better and make them more accessible and easier to analyze. Due to the availability of a large amount of data from different sources with different formats, data wrangling has become an essential part of data modeling which helps data scientists fasten the decision-making process.  

Here below [6 steps data wrangling](https://favtutor.com/blogs/data-wrangling):

1. **Discovering**: The first step in data wrangling is analyzing the data before imputing the data. Wrangling needs to be done in a systematic fashion, based on some criteria which could demarcate and divide the data accordingly – these are identified in this step.

2. **Structuring**: In most cases, the raw data extracted as user information generally doesn’t have structured data. The data should be restructured in a fashion that better suits the analytical method used. Based on the category identified in the first step, the data should be segregated to make use easier. For better analysis we have to select one column that may become two or rows may be split, this is also called feature engineering.

3. **Cleaning**: Processed datasets definitely have some outliers, which can skew the results of the analysis. The dataset should be cleaned for optimum results. In this step, the data is cleaned thoroughly for high-quality data analysis. Null values should be imputed, and the formatting will be standardized to create higher quality processed data

4.  **Enriching**: After processing the data, it will have to be enriched – this is performed in the fourth step. This implies that you one has to take stock of what is in the data and strategize whether you have upscale, downsample, or perform data augmentation. There are different methods to resample the data, one downsampling the data, and the other creating synthetic data using upsampling.

5. **Validating**: Validation refers to iterative programming steps that are used to verify the consistency and the quality of data after processing. For example, you will have to ascertain whether the fields in the data set are accurate via validating data, or checking whether the attributes are normally distributed.

6. **Publishing**: The processed and wrangled data is published so that it can be used further – which is the sole purpose of data wrangling. If needed, the complete data wrangling process should be documented efficiently for the users and clients for easy usage.

<div style="margin-bottom:20px;">
</div>

```{r echo=FALSE, out.width='100%', fig.align="center"}
knitr::include_graphics("E:\\Dropbox\\GitHub\\chemstat-r-github.io\\Image\\PNG_FILE_02\\data_wrangling_background.png")
```

<div style="margin-bottom:20px;">
</div>




## **Important R Packages for Data Wrangling**


* [dplyr](https://dplyr.tidyverse.org/index.html) 

* [purr](https://purrr.tidyverse.org/)

* [tidyr](https://tidyr.tidyverse.org/)

* [lubridate](https://lubridate.tidyverse.org/)

* [magrittr](https://magrittr.tidyverse.org/)

* [plyr](http://plyr.had.co.nz/)

* [purrrlyr](https://cran.r-project.org/web/packages/purrrlyr/index.html)

* [data.table](https://rdatatable.gitlab.io/data.table/)

* [sqldf](https://cran.r-project.org/web/packages/sqldf/index.html)

* [dtplyr](https://www.tidyverse.org/blog/2019/11/dtplyr-1-0-0/)

* [DT](https://rstudio.github.io/DT/)



[dplyr](https://dplyr.tidyverse.org/index.html), [purr](https://purrr.tidyverse.org/), [tidyr](https://tidyr.tidyverse.org/), [lubridate](https://lubridate.tidyverse.org/), and [magrittr](https://magrittr.tidyverse.org/) are installed with [tidyverse](https://www.tidyverse.org/packages/) package as  discussed before. 

<div style="margin-bottom:20px;">
</div>

## **Data Processing with dplyr and tidyr**

 
The **dplyr**  and **tidyr** are the most commonly use data manipulation packages, providing a set of functions that help us solve the most common data manipulation challenges. These are:
 
* **mutate()** adds new variables that are functions of existing variables

* **select()** picks variables based on their names.

* **filter()** picks cases based on their values.

* **summarise()**  reduces multiple values down to a single summary.

* **arrange()** changes the ordering of the rows.

* **join()** join two data-frames

* **group_by()** All above combine naturally with group_by() which allows you to perform any operation “by group”. 
 
 <div style="margin-bottom:40px;">
</div>
 
Here below data Wrangling with [dplyr and tidyr Cheat Sheets](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf):

<div style="margin-bottom:20px;">
</div>

```{r echo=FALSE, out.width='100%', fig.align="center"}
knitr::include_graphics("E:\\Dropbox\\GitHub\\chemstat-r-github.io\\Image\\PNG_FILE_02\\data_wragling cheat_sheet_01.png")
```


```{r echo=FALSE, out.width='100%', fig.align="center"}
knitr::include_graphics("E:\\Dropbox\\GitHub\\chemstat-r-github.io\\Image\\PNG_FILE_02\\data_wragling cheat_sheet_02.png")
```
<div style="margin-bottom:20px;">
</div>


 
### **Load Package**
 
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
library(DT)
#require(plyr)
# library(dplyr)
# library(readr)
```
<div style="margin-bottom:20px;">
</div>


###  **Load data**

In this exercise  we will use two data sets: USA county level lung cancer mortality rates with risk factors from 1998 t0 2012 and USA  County FIPS Codes. We will use **read_csv()** function of **readr** package to import data as a **Tidy** data. All data set use in this exercise could be found [here](https://www.dropbox.com/sh/ctjv2eiifhfh2ts/AAAVXmWxPntD-O-5AE_YH3VHa?dl=0).


```{r message=FALSE, warning=FALSE}
## Create a data folder
dataFolder<-"E:/Dropbox/GitHub/chemstat-r-github.io/Data/"
df<-read_csv(paste0(dataFolder,"LBC_data.csv"))
fips<-read_csv(paste0(dataFolder, "US_FIPS.csv"))
regions<-read_csv(paste0(dataFolder, "US_regions.csv"))
```
<div style="margin-bottom:20px;">
</div>

We examine structure of the data: 

```{r}
str(df); 
```
<div style="margin-bottom:20px;">
</div>

take a **glimpse** of the df dataset:

```{r}
glimpse(df)
```



### **Pipe Operator**

Before starting this tutorial, I like to brief you about the **Pipe Operator**. This is the most important operator for data wrangling in R. The pipe operator, written as **%>%**, has been a longstanding feature of the **magrittr** package for R.  It takes the output of one function and passes it into another function as an argument. This allows us to link a sequence of analysis steps.  In this tutorial,  you can see exactly how this works in a real example.  (Source:https://towardsdatascience.com/an-introduction-to-the-pipe-in-r-823090760d64)  

#
## **Join**

In R we use **base::merge()** function to merge two dataframes. This function is present inside **join()** function of dplyr package. The most important condition for joining two dataframes is that the column type should be the same of “key” variable by  the merging happens. Types of **base::merge()** and **join()** function of dplyr available in R are:


```{r echo=FALSE, out.width='100%', fig.align="center"}
knitr::include_graphics("E:\\Dropbox\\GitHub\\chemstat-r-github.io\\Image\\PNG_FILE_02\\join_functions.png")
```
<div style="margin-bottom:20px;">
</div>

The advantages of the specific dplyr verbs is that they It automatic identify the common "key" columns of two data-frames. dplyr’s joins are considerably faster and don’t mess with the order of the rows. 
(source: https://r4ds.had.co.nz/relational-data.html) . 


First we will join fips and regions file using *inner_join*()* and then join df files. 

```{r}
fips_df = inner_join(fips, regions)
head(fips_df)
```

```{r}
mf = inner_join(fips_df, df)
head(mf)
```


Now we will move  REGION, DIVISION column after FIPS for organizing the columns in the following order: 
FIPS, REGION, DIVISION,  STATE, COUNTY, X, Y, Year, SMOKING, POVERTY, PM25, NO2, SO2, LBC_RATE.  We will use **relocate()** function: 

```{r}
mf<-relocate(mf, REGION,   DIVISION, .after = FIPS)
head(mf)
```

Now explore regions names with **levels()**

```{r}
levels(as.factor(mf$REGION))
```
<div style="margin-bottom:20px;">
</div>

### **Filter**

The **filter()** function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions. Note that when a condition evaluates to NA the row will be dropped, unlike base subsetting with

There are many functions and operators that are useful when constructing the expressions used to filter the data:

* ==, >, >= etc

* &, |, !, xor()

* is.na()

* between(), near()



We will use **filter()**  to extract data only for northeast (single criteria)

```{r}
mf.northeast<-mf %>% filter(REGION == "Northeast")
levels(as.factor(mf.northeast$REGION))
```


Filtering by multiple criteria within a single logical expression - select both Northeast and Midwest

```{r}
mf.north_mid_01<- mf %>% filter(REGION %in%c("Northeast","Midwest"))
levels(as.factor(mf.north_mid_01$REGION))
```


or we can use  **|** which represents **OR** in the logical condition, any of the two conditions.

```{r}
mf.north_mid_02<- mf %>% filter(REGION == "Northeast" | REGION == "Midwest")
levels(as.factor(mf.north_mid_02$REGION))
```


Following filter create a  files for the Northeast region only with NY state. 

```{r}
mf.ny<-mf %>% filter(REGION == "Northeast" & STATE == "New York")
```


Following filters will select counties rows where `LBC_RATE` is greater than the global average of LBC - rate:

```{r}
mean.LBC <- mf %>% filter(LBC_RATE > mean(LBC_RATE, na.rm = TRUE))
```


We will **&** in the following filters to  select counties or rows where `LBC_RATE` is greater than the global average of for the year 2012

```{r}
mean.LBC.2012 <- mf %>% filter(LBC_RATE > mean(LBC_RATE, na.rm = TRUE) & Year ==2012)
```


Following command will select counties start with "A". filter() with **grepl()** is used to search for pattern matching.


```{r}
counties.a <- mf %>% filter(grepl("^A", COUNTY))
head(counties.a)
```
<div style="margin-bottom:20px;">
</div>


### **select**


**dplyr** selections implement a dialect of R where operators make it easy to select variable

* **:** for selecting a range of consecutive variables.

* **!** for taking the complement of a set of variables.

* **&** and **|** for selecting the intersection or the union of two sets of variables.

* **c()** for combining selections.


#### Selecting Variables (or Columns)

Following will create a dataframe with FIPS, Year and LBC rate

```{r}
LBC.df <- mf %>% select(FIPS, Year, LBC_RATE)
head(LBC.df)
```

Following will create a dataframe with FIPS, Year all risk factors (SMOKING, POVERTY, PM25, NO2, SO2)

```{r}
riskfactors.df <- mf %>%  select(FIPS, Year, SMOKING:SO2)
head(riskfactors.df)
```

Following command will give same output.  The minus sign before a variable tells R to drop the variable or variables.  **-(REGION:Y)**, **-LBC_RATE**   will drop these columns: REGION, DIVISION, STATE,  COUNTY, X, Y and LBC_RATE.

```{r}
riskfactors_01.df= mf %>% select(-(REGION:Y), -LBC_RATE)
head(riskfactors_01.df)
```

The following functions helps you to select variables based on their names.

```{r echo=FALSE, out.width='80%', fig.align="center"}
knitr::include_graphics("E:\\Dropbox\\GitHub\\chemstat-r-github.io\\Image\\PNG_FILE_02\\select.png")
```
<div style="margin-bottom:20px;">
</div>

#### **Rename**

The rename function can be used to rename variables.


```{r}
mf.new <- mf %>% 
        rename("LBC_Mortality_Rate" = "LBC_RATE")
names(mf.new)
```
<div style="margin-bottom:20px;">
</div>


### **Arrange**


**arrange()** orders the rows of a data frame by the values of selected columns.


```{r}
# arrange ascending order as default
mf.arrange.ac<- mf %>% arrange(SMOKING) 
head(mf.arrange.ac$SMOKING)
```

```{r}
# arrange descending order
mf.arrange.de<- mf %>% arrange(desc(SMOKING)) 
head(mf.arrange.de$SMOKING)
```


### **Combined multiple functions with Pipes**

In this exercise will use **select()**,  *filter()* and **rename()** in a single commands with **%>%**. First we select data FIPS, SATE, LBC rate and then filter out the data NY state data for the year 2012 and then rename LBC_RATE column.  


```{r}
LBC.NY <- mf %>% select(FIPS, Year, STATE, LBC_RATE) %>%
                 filter (STATE=="New York", Year == 2012)  %>%
                 rename("LBC Mortality Rate"= LBC_RATE)
                 
head(LBC.NY)        
```

### **Pivoting dataframe**

**pivot_longer()** and **pivot_wider()** are very flexible, and can easily tidy a wide variety of non-tidy datasets


#### **pivot_wider**

pivot_wider() convert  a dataset wider by increasing the number of columns and decreasing the number of rows.


```{r}
LBC.wider<- mf %>% select (FIPS, Year, STATE, COUNTY, LBC_RATE) %>%
                  pivot_wider(names_from = Year, # column need to be wider
                              values_from = LBC_RATE) # data
str(LBC.wider)
```

#### **pivot_longer**


The pivot_longer() function can be used to pivot a data frame from a wide format to a long format.

```{r}
names(LBC.wider)
```

```{r}
LBC.longer<- LBC.wider %>% 
              pivot_longer(cols= c("1998", "1999", "2000", "2001", "2002",
                                    "2003", "2004", "2005", "2006", "2007",
                                    "2008", "2009", "2010", "2011", "2012"),
                          names_to = "Year", # column need to be wider
                          values_to = "LBC_RATE") # data
str(LBC.longer)
```
<div style="margin-bottom:20px;">
</div>


### **Mutate**

**mutate()** – adds new variables while retaining old variables to a data frame. In this exerciser, we will add log10 of LBC rate to mf data-frame. 

```{r}
mf.new<- mf %>% mutate(logLBC = log10(LBC_RATE))
head(mf.new[9:15])
```


Below series of codes, will create a wider datafrme of LBC mortality rate than  add three additional columns, such as Mean, Median and SD which are represents yearly (1998-2012) mean, median and standard deviation of  LBC mortality rates. Before **mutate()** we use **rowwise()** function which  allows us to compute on a dataframe a row-at-a-time, in our cases all 3107 counties. 


```{r}
LBC.wider.summary<- mf %>% select (FIPS, Year, STATE, COUNTY, LBC_RATE) %>%
                  pivot_wider(names_from = Year, # column need to be wider
                  values_from = LBC_RATE) %>%
                  rowwise() %>%
                  mutate(Mean = mean(c_across(4:18)),
                         Median= median(c_across(4:18)),
                         SD= sd(c_across(4:18))) # calculates the median 
head(LBC.wider.summary)
```
<div style="margin-bottom:20px;">
</div>

### **Summarize**


**summarise()** creates a new data frame. It will have one (or more) rows for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.

**summarise()** and **summarize()** are synonyms.


* **Center**: mean(), median()

* **Spread**: sd(), IQR(), mad()

* **Range**: min(), max(), quantile()

* **Position**: first(), last(), nth(),

* **Count**: n(), n_distinct()

* **Logical**: any(), all()


```{r}
# mean
summarise(mf, Mean=mean(LBC_RATE))
# median
summarise(mf, Median=median(LBC_RATE))
```
<div style="margin-bottom:20px;">
</div>


The **scoped variants (_if, _at, _all)** of summarise() make it easy to apply the same transformation to multiple variables. There are three variants.


* summarise_all() affects every variable

* summarise_at() affects variables selected with a character vector or vars()

* summarise_if() affects variables selected with a predicate function

Following series of codes create a **datatable** of summary statistics of LBC_RATE, SMOKING, POVERTY, PM25, NO2, SO2 variables. 

```{r}

summary_statistics <- mf %>% 
  # First select  numerical columns
  select(LBC_RATE, SMOKING, POVERTY, PM25, NO2, SO2) %>%
  # rename LBC_rate to LBC
  dplyr::rename("LBC"= LBC_RATE) %>%
  # get summary statistics
   summarise_all(funs(min = min, 
                      q25 = quantile(., 0.25), 
                      median = median, 
                      q75 = quantile(., 0.75), 
                      max = max,
                      mean = mean, 
                      sd = sd)) %>% 
# create a nice looking table
  pivot_longer(everything(), names_sep = "_", names_to = c( "variable", ".value"))

```

We can visualize the summary_statistics using **datatable()** function of **DT** package DT which provides an R interface to the JavaScript library DataTables.   *datatable()** creates an HTML widget to display R data objects with DataTables.  

```{r}
datatable(as.data.frame(summary_statistics), 
          rownames = T, options = list(pageLength = 10, scrollX = TRUE, round)) %>%
  formatRound(columns = 2:8, digits = 3)
```

### **Group by**

Now, we can apply the **group_by()** and **summarize()** functions to calculate mean all variables  by States:


```{r}
state.mf<-mf %>% 
  group_by(STATE) %>%
  summarize(LBC_RATE = mean(LBC_RATE),
            SMOKING = mean(SMOKING),
            POVERTY = mean(POVERTY),
            PM25 = mean(PM25),
            NO2 = mean(NO2),
            SO2 = mean(SO2))
```


```{r}
datatable(as.data.frame(state.mf), 
          rownames = T, options = list(pageLength = 10, scrollX = TRUE, round)) %>%
  formatRound(columns = 2:7, digits = 3)
```
<div style="margin-bottom:30px;">
</div>

## **Cleaning a Messy Data**   

In this exercise you will learn how a clean data and make it ready for analysis, such as dealing with missing values, data with below detection limit and cleaning spatial characters. We will use data from United States Geological Survey (USGS) as a part of the USGS Geochemical Landscapes Project [(Smith et al., 2011)](https://pubs.usgs.gov/ds/801/).

#### Load the data 

```{r message=FALSE, warning=FALSE}
## Create a data folder
dataFolder<-"E:/Dropbox/GitHub/chemstat-r-github.io/Data/"
df.geo<-read_csv(paste0(dataFolder,"usa_geochemical.csv"))
meta.geo<-read_csv(paste0(dataFolder,"usa_geochemical_meta_data.csv"))
```

```{r}
glimpse(df.geo)
```

```{r}
glimpse(meta.geo)
```


We first create a file with limited number of variables using **select** functions


```{r}
mf.geo <- df.geo %>%
      select(A_LabID, StateID,  LandCover1, A_Depth, A_C_Tot, A_C_Inorg, A_C_Org, A_As, A_Cd,  A_Pb, A_Cr) %>%
      rename("LAB_ID"=A_LabID,
             "LandCover" =LandCover1,
             "Soil_depth" = A_Depth,
             "Total_Carbon" = A_C_Tot,
             "Inorg_Carbon" = A_C_Inorg,
             "Organic_Carbon"= A_C_Org, 
             "Arsenic" = A_As,
             "Cadmium" = A_Cd,
             "Lead" = A_Pb,
             "Chromium" = A_Cr) %>% glimpse()
```


Now, we filter the data where records have N.S. values INS: 

```{r}
mf.geo<- mf.geo %>% 
        filter(Soil_depth != "N.S." & Total_Carbon !="INS")
```


Then, we will convert all N.D. values to empty string: 


```{r}
mf.geo[mf.geo=="N.D."]<- ""
```

Here detection limits of As, Cd, Pb and Cr are  0.6, 0.1, 0.5, and 1 mg/kg, respectively. We will replace  the  values below detection limits by half of detection limits of these heavy-metals. Before that we have to remove **"<"** and convert the all **<chr>** to **<double>**.


```{r}
mf.geo <- mf.geo %>%
      mutate_at(c("Arsenic", "Cadmium", "Lead", "Chromium"), str_replace, "<", "") %>%
      mutate_at(c(5:11), as.numeric) %>% glimpse()
```


Now replace values below detection limits:


```{r}
mf.geo["Arsenic"][mf.geo["Arsenic"] == 0.6] <- 0.3
mf.geo["Cadmium"][mf.geo["Cadmium"] == 0.1] <- 0.05
mf.geo["Lead"][mf.geo["Lead"] == 0.5] <- 0.25
mf.geo["Chromium"][mf.geo["Chromium"] == 1] <- 0.5
```


Now will check the missing values of the data:

```{r}
# counting unique, missing, and median values
Arsenic<- mf.geo %>% summarise(N = length(Arsenic),
                 na = sum(is.na(Arsenic)),
                 Min = min(Arsenic, na.rm = TRUE),
                 Max =max(Arsenic, na.rm = TRUE))

Cadmium<- mf.geo %>% summarise(N = length(Cadmium),
                 na = sum(is.na(Cadmium)),
                 Min = min(Cadmium, na.rm = TRUE),
                 Max =max(Cadmium, na.rm = TRUE))

Lead<- mf.geo %>% summarise(N = length(Lead),
                 na = sum(is.na(Lead)),
                 Min = min(Lead, na.rm = TRUE),
                 Max =max(Lead, na.rm = TRUE))

Chromium<- mf.geo %>% summarise(N = length(Chromium),
                 na = sum(is.na(Chromium)),
                 Min = min(Chromium, na.rm = TRUE),
                 Max =max(Chromium, na.rm = TRUE),
                 )

#bind the data
geo.sum<- bind_rows(Arsenic, Cadmium, Lead, Chromium)

#add.row.names
row.names(geo.sum) <- c("Arsenic", "Cadmium", "Lead", "Chromium")
datatable(geo.sum)
```

One of the common method of replacing missing values  is **na.omit**. The function **na.omit()** returns the object with listwise deletion of missing values and function create new dataset without missing data.


```{r}
newdata <- na.omit(mf.geo)
glimpse(newdata)
```

This function delete all  records with missing values. Now newdata have only 1,167 records, since Inorg_Carbon variable has 3,690 missing values which all are omitted. 

We can impute missing values in several ways. The easiest way to impute missing values is by replacing the mean values of the variable. The following  code replace missing of Arsenic, Cadmium, Lead and  Chromium with their mean values. 


```{r}
mf.geo.new<-mf.geo %>% mutate_at(vars(Arsenic, Cadmium, Lead, Chromium),~ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)) %>% glimpse()
```

```{r}
mf.geo.new %>% summarise(sum(is.na(Chromium)))
```

<div style="margin-bottom:40px;">
</div>

## **Further Reading**


1. [R for Data Science](https://r4ds.had.co.nz/) 

2. [Data Wrangling with R](https://cengel.github.io/R-data-wrangling/)

3. [Book: Data Wrangling with R](https://link.springer.com/book/10.1007/978-3-319-45599-0)

4. [PDF Data wrangling with R and RStudio](https://github.com/rstudio/webinars/blob/master/05-Data-Wrangling-with-R-and-RStudio/wrangling-webinar.pdf)

5. [Youtube Data Wrangling with R and the Tidyverse - Workshop](https://www.youtube.com/watch?v=CnY5Y5ANnjE)
 
 